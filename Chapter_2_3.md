# 2.3 知識治理與隱私堡壘：權限分配、數據脫敏與安全框架

當企業開始將數據資產化（2.1）並透過 RAG 讓 AI 能夠隨時檢索（2.2）時，一個巨大的幽靈隨之而來：**安全性與隱私問題**。如果一個基層員工透過 AI「不經意地」問出了全公司的高層薪資結構，或是 AI 在回覆客戶詢問時意外洩露了另一位客戶的採購清單，這對企業來說將是災難性的。

因此，在知識工程的最後一哩路，我們必須建立一套嚴謹的**「知識治理 (Knowledge Governance)」**機制。

### 1. 權限邊界的數位化：誰能看到什麼？
傳統企業的權限管理是基於資料夾或伺服器的，但 AI 時代的權限管理必須精細到「語義層級」。
- **檢索前過濾 (Pre-retrieval filtering)**：在 RAG 檢索前，系統必須先確認該使用者的身份（Identity）。只有當使用者俱備該文件的「已授權標籤 (Identity Tags)」時，向量數據庫才會將該知識區塊返回。
- **Agent 的身分限制**：不只是人，連 Agent 都應該有權限。例如，負責「對外行銷」的 Agent，其知識庫路徑應該完全隔離於「研發機密」之外。

### 2. 數據脫敏與去識別化 (Data Masking & PII Redaction)
在將數據存入向量庫或發送給公有雲模型（如 OpenAI）之前，必須經過一道「洗滌」程序。
- **敏感資訊掃描 (PII Scanning)**：自動識別並遮蔽姓名、身分證字號、信用卡號或特定的商業代碼。
- **語義保留脫敏**：進階的脫敏技術會將敏感資訊轉化為佔位符（如：[CLIENT_NAME_A]），這樣 AI 仍能理解其中的邏輯關係（如：訂單金額、履約日期），但不會知曉具體的主體是誰。

### 3. AI 倫理與邊界設定 (Safety Guardrails)
除了隱私，企業還必須設定 AI 的「言論防護欄」。
- **輸出過濾 (Output Guardrails)**：利用專門的模型（如 Llama Guard 或特定的安全插件）來檢測 AI 的回覆。如果 AI 被引導去評論政治、競爭對手，或是給出未經授權的法律建議，系統應在輸出抵達使用者端前進行截斷。
- **可解釋性與審計追蹤 (Audit Trail)**：對於 Agent 做的每一項決定，系統都必須記錄：它檢索了哪些參考文件？它的推理路徑為何？這對於日後的錯誤排查與法律合規審查至關重要。

### 4. 混合雲與本地部署的抉擇 (Hybrid Security Strategy)
對於極度敏感的核心知識（如專利、軍工、金融交易核心），企業應考慮將「Embedding 運算」與「向量存儲」放在本地（On-premise），僅將去識別化後的 Prompt 發送到雲端模型，或者直接使用本地部署的開源模型（如 Llama 3 8B/70B）。

### 結論
知識治理不應該是創新的絆腳石，而是創新的保險絲。一個缺乏治理的 AI 系統就像是一個隨時會洩密的實習生，沒人敢真正賦予它重任。只有建立了「權限分明、運算透明、數據安全」的隱私堡壘，企業才能放心地將核心業務交給 AI 代理人去執行。

下一節 2.4，我們將針對這整章的技術底座進行一次總結自測，確保您的企業腦袋已經「裝修完成」。
